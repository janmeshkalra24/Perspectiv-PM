 Good morning everyone, hope you're all doing well and thank you for being here today. So today I'll be walking you through the improvements we've made to our python script and how we've and how it has evolved from to its current stage. So let's start with how things were before. Our old python code was basically designed to process user data from a csv file. A csv file is essentially a spreadsheet stored as plain text and each row represents a piece of data and the columns separate different attributes like names, dates, whatever. So in a nutshell what our old code did was it opened the csv file and read each row individually and then it processed each row one at a time. Then after processing it saved the output into another file for future use. Although this approach was functional and it worked it had a lot of drawbacks. For starters it was extremely slow. Since it processed every single row one by one it took a long amount of time for large data sets making it that it's not scalable since if we had a data set that had thousands of millions of records the script would take forever to finish. And another thing with csv files is the data can be updated since once a csv file is made it's static. The data is not updated automatically whatever is there is there from before. Now if this were a real world application like something like a weather app or a stock market trader it would only read from an instead of only reading instead of giving you live data it will only read from an old file which wouldn't be very useful because you need that information in its current state. So which is why we needed to improve our code. So moving on to our new code let's talk about that for a bit. So instead of using csv files we now use a proper database which is optimized for data processing and it's also connected to an api. So for the first point of how we're using a database this is better because databases allow us to look up data faster so instead of searching through thousands of thousands of lines in a file one by one a database can help us quickly retrieve specific data from any part of the file. This ensures data consistency since if multiple parts of a program need to access the data they all see the latest and most accurate information. This in turn supports scalability as the data set grows the database can handle it efficiently. And then moving on to data processing instead of processing one row at a time we now use batch operations which means we process multiple records at once. So if you take a look at this code simple bit of our old code you can see we import the csv file and then we use a simple for loop to iterate through each and every single row and they read the data. Excuse me sorry. And this approach in fact is slow because it loops through each and every single line individually. Now if you look at this code snippet of our new version you can see we connect you to the database and multiple you and in this instance we are adding multiple users and their age to the database. This is done at once instead of having to do line by line this can be done at once and this data can be put in or retrieved or whatever you like all at the same time which makes it much faster and more efficient. Now so the third thing for the API this is perhaps the biggest improvement to our code So you might wonder what is an API. An API stands for Application Programming Interface. Think of it as some sort of bridge which lets our program talk to other systems over the internet. For example let's say we were still building that weather app so instead of manually entering the weather data every day we can connect to a weather API and fetch live updates in real time. Now you might wonder how an API works. So our python script sends a request to the API saying let's say give me the latest weather in New York. Then our API responds with fresh data. For example the temperature is 70 degrees Fahrenheit with 60% humidity, 30% chance of precipitation, whatever. And our program will basically store this data as sent by the API in the database. So it can use it in reports and that which data can that data can be used in reports, dashboards, apps, whatever. And here's a basic code snippet. So we give it an API URL. And then we just use a simple get function to call like a request gets function to retrieve that data. And then that data is inserted into a database using the rest. Now this matters because it it makes a new approach so much better. It's faster since batch processing and databases speed things up. It's much more reliable since a proper database prevents data loss and corruption. Since the csv file the entire thing can be corrupted. It's in real time. APIs will allow us to fetch live data. You know we can call the API multiple times and this ensures accuracy and it's also scalable. So if you're not limited to you know thousands of entries in the csv file but databases can handle 100 users or million users whatever it can manage its load efficiently. And you know APIs and this type of data processing used everywhere today whether that's for weather apps, stock market platforms, online stores, to like smart home devices, everything uses API at its core. So to conclude we have transformed our old code which was file based into a new API driven and database powered system making it more efficient, more scalable, and more useful in the real world. Yep, that's it. If you guys have any questions let me know. Thank you.
